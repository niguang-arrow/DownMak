# 机器学习相关问题

2018 年 3 月 25 日

[TOC]



## 机器学习



### 过拟合

<统计学习方法> 11 页

过拟合指的是学习时选择的模型所包含的参数过多, 以至于其对已知数据预测的结果很好, 但是对未知的数据预测得很差的现象.



### 生成模型与判别模型

来自 <统计学习方法>

#### 生成方法

生成方法由数据学习联合概率分布 $P(X, Y)$, 然后求出条件概率分布 $P(Y|X)$ 作为预测的模型. 生成方法还原联合概率分布 $P(X, Y)$, 表示了给定输入 X 产生输出 Y 的生成关系.

典型的生成模型有: 朴素贝叶斯, 隐马尔科夫模型.



#### 判别方法

判别方法由数据直接学习决策函数 $f(x)$ 或条件概率分布 $P(Y|X)$ 作为预测的模型. 判别模型关心的是给定输入 X 应该预测什么样的输出 Y. 

常见的模型: k近邻, 感知机, 决策树, logistic 回归, SVM, 最大熵, boost(提升)方法, 条件随机场.



### 朴素贝叶斯

#### 一. 参考资料

[机器学习实战教程（四）：朴素贝叶斯基础篇之言论过滤器](http://cuijiahua.com/blog/2017/11/ml_4_bayes_1.html)

#### 二. 介绍

朴素贝叶斯算法是有监督的学习算法, 它属于生成模型, 它主要用来解决分类问题. 该算法的优点是简单易懂, 学习效率高, 在小规模数据上表现很好, 适合多分类任务, 但是该算法以自变量之间的特征条件独立性和连续变量的正态性假设为前提, 会导致算法的精度受到影响. (**先总体概括, 再简单细分, 之后分析优缺点, 最后深入细节进行讨论.**)

算法的具体步骤是:

对于给定的训练集:

1. 由特征条件独立性假设学习输入输出的联合概率分布;
2. 之后基于此模型, 对于给定的输入 $x$, 利用贝叶斯定理求出后验概率最大的输出 $y$.



(对于连续变量的处理, 可以假设特征服从正态分布, 因此, 首先通过特征计算出均值和方差, 然后根据正态分布的概率密度函数求得每个特征的条件概率密度(计算出来的概率密度可能大于 1, 但是没有关系, 详见 [朴素贝叶斯分类器的应用](http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html) 最后一个例子), 再根据特征条件独立性假设, 获得输入输出的联合概率分布, 最后基于贝叶斯定理求得使后验概率最大的输出 y.)



在博客 [机器学习实战教程（五）：朴素贝叶斯实战篇之新浪新闻分类](http://cuijiahua.com/blog/2017/11/ml_5_bayes_2.html) 中指出了上面的朴素贝叶斯算法在实际实现中, 会出现以下两个问题:

1. 一方面由于特征条件独立性假设, 计算条件概率时是多个概率进行乘积, 如果其中某个概率为 0 的话, 那么就会对后验概率的计算产生极大的印象, 使分类结果不正确. (解决的办法是使用贝叶斯估计, 一般使用拉普拉斯平滑)
2. 另一方面是如果各个概率都比较小, 那么大量很小的数进行乘积时, 容易出现下溢出的问题(因为结果越乘越小). 在程序中, 相应的小数进行四舍五入, 计算结果可能就变为 0 了. 为了解决这个问题, 可以对乘积的结果取自然对数, 通过求对数, 可以避免下溢出或者由于浮点数的四舍五入导致的错误.



##### 贝叶斯估计

来自 <统计学习方法>

使用极大似然估计可能会出现所要估计的概率值为 0 的情况(毕竟我们使用多个条件概率相乘, 如果其中某个值为 0, 那么结果就变为 0 了). 这时会影响到后验概率的计算结果, 使得分类产生偏差. 解决这一问题的办法是使用贝叶斯估计. 

其中 $\lambda = 1$ 时, 称为拉普拉斯平滑.

另外注意条件概率的贝叶斯估计与先验概率的贝叶斯估计公式的不同的, 要特别注意分母!



下面的内容为基础知识: 

##### 贝叶斯决策理论

贝叶斯决策理论的核心思想是: 我们会选择高概率对应的类别. 比如得到 `p1(x1, x2) > p2(x1, x2)`, 那我们就把样本 `(x1, x2)` 归为类别 1.

##### 贝叶斯推断

对条件概率公式进行变形, 我们可以得到:
$$
P(A|B) = P(A)\frac{P(B|A)}{P(B)}
$$
我们把 $P(A)$ 称为 "先验概率" (Prior probability)

$P(A|B)$ 称为 "后验概率" (Posterior probability)

$P(B|A)/P(B)$ 称为 "可能性函数" (Likelyhood), 这是一个调整因子, 使得预估概率更接近真实概率.

所以，条件概率可以理解成下面的式子：

```bash
后验概率　＝　先验概率 ｘ 调整因子
```

**这就是贝叶斯推断的含义。我们先预估一个"先验概率"，然后加入实验结果，看这个实验到底是增强还是削弱了"先验概率"，由此得到更接近事实的"后验概率"。**

在这里，如果"可能性函数" $P(B|A)/P(B)>1$，意味着"先验概率"被增强，事件 A 的发生的可能性变大；如果"可能性函数"=1，意味着 B 事件无助于判断事件 A 的可能性；如果"可能性函数"<1，意味着"先验概率"被削弱，事件A的可能性变小。

在进行贝叶斯推断时, 并不需要计算全概率(即上面的 $P(B)$ 不需要计算), 只需要比较 `P(B|A1)P(A1)` 与 `P(B|A2)P(A2)` 的大小, 然后判断给定 B 时, 样本属于 A1 还是 A2.

##### 朴素贝叶斯推断

朴素贝叶斯中的 "朴素" 含义在于: 它对条件概率分布做了条件独立性的假设. 即:
$$
P(y|X) = p(X|y)p(y) = p(x_1, x_2, \ldots, x_n|y)p(y)
$$
由于每个特征都是独立的, 因此我们有:
$$
p(x_1, x_2, \ldots, x_n|y)p(y) = p(x_1|y)*p(x_2|y)*\ldots*p(x_n|y)p(y)
$$


### 决策树

#### 信息熵

熵在信息论中代表随机变量的不确定度的度量.
$$
H = -\sum_{i=1}^{k}p_i\log(p_i)
$$




## 深度学习

### 网络架构

#### AlexNet (2012)

参考资料: [理解卷积神经网络的利器：9篇重要的深度学习论文](https://mp.weixin.qq.com/s/yf7gnKNqQnPSNAMTZmCuqA)

在 2012 年 ImageNet 大规模视觉识别挑战赛取得冠军, 将分类误差从 26% 下降到 15%. AlexNet 的网络结构相对比较简单, 它由 5 个卷积层, 最大池化层, dropout 层和 3 个全连接层组成.

主要论点有:

1.  在 ImageNet 上训练网络, ImageNet数据集包含超过1500万张注释图像，类别超过22000个。
2.  使用ReLU处理非线性函数（这样可以减少训练时间，因为ReLU比传统的tanh函数运行速度要快几倍）, 一方面解决梯度消失的问题, 另一方面由于 ReLU 只需要设置阈值, 而 tanh 或 Sigmoid 还需要进行指数运算, 因此可以减少运算时间.
3.  使用数据增强技术包括: 图像转换, 水平翻转以及提取图像子块.
4.  采用 Drop-out 层, 解决了训练数据的过拟合问题.
5.  使用批量随机梯度下降的训练架构, 其中动量和权重衰减都有固定的具体值.